{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqBOev1lCayq+y1VD7hADP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo3r_MFqL28W"
      },
      "source": [
        "!pip install numpy nltk matplotlib pdfplumber sklearn pdf2image pytesseract Image gensim\n",
        "!apt install tesseract-ocr tesseract-ocr-por poppler-utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dYP0SrVMQCB"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOJ3f7O8MmjK"
      },
      "source": [
        "#incializando funções\n",
        "\n",
        "from pdf2image import convert_from_path as cPdf2img\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string as img2str\n",
        "import re\n",
        "\n",
        "#################################################\n",
        "page_id = '-'*100+'\\nPage {}\\n'+'-'*100+'\\n'\n",
        "\n",
        "#regex expressions\n",
        "regexes = [\n",
        "  [r'(\\n)([a-z]|-)', r'\\2'],\n",
        "  [r'(\\s+)(\\.|,|;)', r'\\2'],\n",
        "  [r'\\s+', r' ']\n",
        "]\n",
        "##################################################\n",
        "\n",
        "#usando pdfplumber\n",
        "def usePdfplumber(filename, verbose=True):\n",
        "  if verbose:\n",
        "    print('Converting \"{}\" using pdfplumber'.format(filename))\n",
        "  fd = open(filename[:-4] + '_pplumber.txt', 'w')\n",
        "  i = 0\n",
        "  with pdfplumber.open(filename) as pdf:\n",
        "    n_pages = len(pdf.pages)\n",
        "    for page,i in zip(pdf.pages,range(n_pages)):\n",
        "      print(\"Converting page #{}\".format(i+1))\n",
        "      text = page.extract_text()\n",
        "      for rPattern,subString in regexes:\n",
        "        text = re.sub(rPattern,subString,text)\n",
        "      fd.write(page_id.format(i+1))\n",
        "      fd.write(text + '\\n')\n",
        "  fd.close()\n",
        "\n",
        "#usando pytesseract\n",
        "def usePytesseract(filename, verbose=True):\n",
        "  if verbose:\n",
        "    print('Converting \"{}\" using pytesseract'.format(filename))\n",
        "  fd = open(filename[:-4] + '_ptesseract.txt', 'w')\n",
        "  pages =cPdf2img(filename)\n",
        "  for i,page in enumerate(pages):\n",
        "    print(\"Converting page #{}\".format(i+1))\n",
        "    text = img2str(page, lang='por')\n",
        "    for rPattern,subString in regexes:\n",
        "        text = re.sub(rPattern,subString,text)\n",
        "    fd.write(page_id.format(i+1))\n",
        "    fd.write(text + '\\n')\n",
        "  fd.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5iVlHahMw_c"
      },
      "source": [
        "import os\n",
        "\n",
        "files = [filename for filename in os.listdir() if filename[-3:] == 'pdf']\n",
        "\n",
        "for filename in files:\n",
        "  usePdfplumber(filename)\n",
        "  usePytesseract(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMrbbHgbUGCZ"
      },
      "source": [
        "files = [filename for filename in os.listdir() if filename[-3:] == 'txt']\n",
        "f_pplumber = [filename for filename in files if re.match('.+_pplumber\\.txt',filename)]\n",
        "f_ptesseract = [filename for filename in files if re.match('.+_ptesseract\\.txt',filename)]\n",
        "\n",
        "print('Pdf Plumber files\\n\\t{}'.format(f_pplumber))\n",
        "print('Py Tesseract files\\n\\t{}'.format(f_ptesseract))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvRdjV3SSqcD"
      },
      "source": [
        "#tokenizers\n",
        "\n",
        "#word tokenization\n",
        "def w_tokenize(filename):\n",
        "  fd = open(filename,'r')\n",
        "  text = None\n",
        "  for line in fd:\n",
        "    if not re.search('-{2,}',line) and not re.match('Page\\ [0-9]+$',line):\n",
        "      if text:\n",
        "        text += line\n",
        "      else:\n",
        "        text = line\n",
        "  fd.close()\n",
        "  return text.split()\n",
        "\n",
        "#sentence builder\n",
        "def build_sents(t_text):\n",
        "  text_sent = []; sentence = []\n",
        "  for w in t_text:\n",
        "    sentence.append(w)\n",
        "    if w[-1] == '.':\n",
        "      sentence = ' '.join(sentence)\n",
        "      text_sent.append(sentence)\n",
        "      sentence = []\n",
        "  return text_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_y1CVCOyMT"
      },
      "source": [
        "#programa wordninja, colocado explicitamente no programa para\n",
        "#inserção de modifiações específicas para tratamento de palavras\n",
        "#em português\n",
        "#programa original em https://github.com/keredson/wordninja\n",
        "\n",
        "import gzip, os, re\n",
        "from math import log\n",
        "\n",
        "\n",
        "__version__ = '2.0.0'\n",
        "\n",
        "\n",
        "# I did not author this code, only tweaked it from:\n",
        "# http://stackoverflow.com/a/11642687/2449774\n",
        "# Thanks Generic Human!\n",
        "\n",
        "\n",
        "# Modifications by Scott Randal (Genesys)\n",
        "#\n",
        "# 1. Preserve original character case after splitting\n",
        "# 2. Avoid splitting every post-digit character in a mixed string (e.g. 'win32intel')\n",
        "# 3. Avoid splitting digit sequences\n",
        "# 4. Handle input containing apostrophes (for possessives and contractions)\n",
        "#\n",
        "# Wordlist changes:\n",
        "# Change 2 required adding single digits to the wordlist\n",
        "# Change 4 required the following wordlist additions:\n",
        "#   's\n",
        "#   '\n",
        "#   <list of contractions>\n",
        "\n",
        "\n",
        "class LanguageModel(object):\n",
        "  def __init__(self, word_file):\n",
        "    # Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
        "    with gzip.open(word_file) as f:\n",
        "      words = f.read().decode().split()\n",
        "    self._wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
        "    self._maxword = max(len(x) for x in words)\n",
        "   \n",
        "\n",
        "  def split(self, s):\n",
        "    \"\"\"Uses dynamic programming to infer the location of spaces in a string without spaces.\"\"\"\n",
        "    l = [self._split(x) for x in _SPLIT_RE.split(s)]\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "\n",
        "  def _split(self, s):\n",
        "    # Find the best match for the i first characters, assuming cost has\n",
        "    # been built for the i-1 first characters.\n",
        "    # Returns a pair (match_cost, match_length).\n",
        "    def best_match(i):\n",
        "      candidates = enumerate(reversed(cost[max(0, i-self._maxword):i]))\n",
        "      return min((c + self._wordcost.get(s[i-k-1:i].lower(), 9e999), k+1) for k,c in candidates)\n",
        "\n",
        "    # Build the cost array.\n",
        "    cost = [0]\n",
        "    for i in range(1,len(s)+1):\n",
        "      c,k = best_match(i)\n",
        "      cost.append(c)\n",
        "\n",
        "    # Backtrack to recover the minimal-cost string.\n",
        "    out = []\n",
        "    i = len(s)\n",
        "    while i>0:\n",
        "      c,k = best_match(i)\n",
        "      assert c == cost[i]\n",
        "      # Apostrophe and digit handling (added by Genesys)\n",
        "      newToken = True\n",
        "      if not s[i-k:i] == \"'\": # ignore a lone apostrophe\n",
        "        if len(out) > 0:\n",
        "          # re-attach split 's and split digits\n",
        "          if out[-1] == \"'s\" or (s[i-1].isdigit() and out[-1][0].isdigit()): # digit followed by digit\n",
        "            out[-1] = s[i-k:i] + out[-1] # combine current token with previous token\n",
        "            newToken = False\n",
        "      # (End of Genesys addition)\n",
        "\n",
        "      if newToken:\n",
        "        out.append(s[i-k:i])\n",
        "\n",
        "      i -= k\n",
        "\n",
        "    return reversed(out)\n",
        "\n",
        "#DEFAULT_LANGUAGE_MODEL = LanguageModel(os.path.join(os.path.dirname(os.path.abspath(__file__)),'wordninja','wordninja_words.txt.gz'))\n",
        "_SPLIT_RE = re.compile(\"[^a-zA-Z0-9ãõÁáÍíÉéÓóÊêÔôÇçÍíÚúÛû\\-']+\")\n",
        "\n",
        "def split(s):\n",
        "  return DEFAULT_LANGUAGE_MODEL.split(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO7hrZDaQ6ZE"
      },
      "source": [
        "#scrapper para coleta de textos genéricos\n",
        "#em português para montagem do dicionário\n",
        "#nescessário para o programa wordninja\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import FreqDist as fdist\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "urls = ['https://pt.wikipedia.org/wiki/Batalha_da_Pra%C3%A7a_da_S%C3%A9',\n",
        "        'https://pt.wikipedia.org/wiki/Nascimento_de_caix%C3%A3o',\n",
        "        'https://pt.wikipedia.org/wiki/Albert_Einstein',\n",
        "        'https://pt.wikipedia.org/wiki/Anticiclone_do_Atl%C3%A2ntico_Sul',\n",
        "        'https://pt.wikipedia.org/wiki/Margem_de_erro'\n",
        "        'https://pt.wikipedia.org/wiki/Eg%C3%ADpcios_dos_Balc%C3%A3s',\n",
        "        'https://pt.wikipedia.org/wiki/J%C3%BAlio_C%C3%A9sar',\n",
        "        'https://pt.wikipedia.org/wiki/Che_Guevara',\n",
        "        'https://pt.wikipedia.org/wiki/Abraham_Lincoln'\n",
        "        'https://pt.wikipedia.org/wiki/Margaret_Thatcher',\n",
        "        'https://pt.wikipedia.org/wiki/Aston_Villa_Football_Club',\n",
        "        'https://pt.wikipedia.org/wiki/Trof%C3%A9u_da_Copa_do_Mundo_FIFA',\n",
        "        'https://pt.wikipedia.org/wiki/Pol%C3%ADtica_de_contrata%C3%A7%C3%A3o_do_The_Rangers_Football_Club',\n",
        "        'https://pt.wikipedia.org/wiki/Arduino',\n",
        "        'https://pt.wikipedia.org/wiki/Motor_de_jogo',\n",
        "        'https://pt.wikipedia.org/wiki/Wikip%C3%A9dia',\n",
        "        'https://pt.wikipedia.org/wiki/Nota%C3%A7%C3%A3o_cient%C3%ADfica',\n",
        "        'https://pt.wikipedia.org/wiki/Orqu%C3%ADdea',\n",
        "        'https://pt.wikipedia.org/wiki/Corrida_A%C3%A9rea_de_Londres_para_Manchester_de_1910',\n",
        "        'https://pt.wikipedia.org/wiki/Sarc%C3%B3fago_de_Hagia_Triada',\n",
        "        'https://pt.wikipedia.org/wiki/Nebulosa_do_Caranguejo',\n",
        "        'https://pt.wikipedia.org/wiki/Arauc%C3%A1ria',\n",
        "        'https://pt.wikipedia.org/wiki/Batata',\n",
        "        'https://pt.wikipedia.org/wiki/Fungi',\n",
        "        'https://pt.wikipedia.org/wiki/Amplitude_interquartil',\n",
        "        'https://pt.wikipedia.org/wiki/Lei_dos_grandes_n%C3%BAmeros',\n",
        "        'https://pt.wikipedia.org/wiki/Movimento_pela_Extin%C3%A7%C3%A3o_Humana_Volunt%C3%A1ria',\n",
        "        'https://pt.wikipedia.org/wiki/Google',\n",
        "        'https://pt.wikipedia.org/wiki/White_Star_Line',\n",
        "        'https://pt.wikipedia.org/wiki/Venda_de_esposas_na_Inglaterra',\n",
        "        'https://pt.wikipedia.org/wiki/Diego_Maradona',\n",
        "        'https://pt.wikipedia.org/wiki/Johan_Cruijff',\n",
        "        'https://pt.wikipedia.org/wiki/Declara%C3%A7%C3%A3o_de_Independ%C3%AAncia_da_Litu%C3%A2nia',\n",
        "        'https://pt.wikipedia.org/wiki/Acidente_nuclear_de_Chernobil',\n",
        "        'https://pt.wikipedia.org/wiki/Opera%C3%A7%C3%A3o_Barbarossa',\n",
        "        'https://pt.wikipedia.org/wiki/Batalha_de_Tuiuti',\n",
        "        'https://pt.wikipedia.org/wiki/Machado_de_Assis',\n",
        "        'https://pt.wikipedia.org/wiki/Caso_Taman_Shud',\n",
        "        'https://pt.wikipedia.org/wiki/Constitui%C3%A7%C3%A3o_do_Imp%C3%A9rio_Romano',\n",
        "        'https://pt.wikipedia.org/wiki/Elei%C3%A7%C3%A3o_especial_para_presidente_da_C%C3%A2mara_dos_Deputados_do_Brasil_em_2016',\n",
        "        'https://pt.wikipedia.org/wiki/Brasil',\n",
        "        'https://pt.wikipedia.org/wiki/Academicismo',\n",
        "        'https://pt.wikipedia.org/wiki/Academismo_no_Brasil',\n",
        "        'https://pt.wikipedia.org/wiki/Universidade_Federal_do_Rio_de_Janeiro',\n",
        "        'https://pt.wikipedia.org/wiki/Lua',\n",
        "        'https://pt.wikipedia.org/wiki/J%C3%BApiter_(planeta)',\n",
        "        'https://pt.wikipedia.org/wiki/Hist%C3%B3ria_da_biologia',\n",
        "        'https://pt.wikipedia.org/wiki/Decl%C3%ADnio_contempor%C3%A2neo_da_biodiversidade_mundial',\n",
        "        'https://pt.wikipedia.org/wiki/Opera',\n",
        "        'https://pt.wikipedia.org/wiki/Vermelho',\n",
        "        'https://pt.wikipedia.org/wiki/F%C3%ADsica',\n",
        "        'https://pt.wikipedia.org/wiki/Aquecimento_global',\n",
        "        'https://pt.wikipedia.org/wiki/Mar',\n",
        "        'https://pt.wikipedia.org/wiki/Olho_(ciclone)',\n",
        "        'https://pt.wikipedia.org/wiki/Crise_da_abdica%C3%A7%C3%A3o_de_Eduardo_VIII',\n",
        "        'https://pt.wikipedia.org/wiki/Parlamento_Europeu',\n",
        "        'https://pt.wikipedia.org/wiki/Compagnie_G%C3%A9n%C3%A9rale_Transatlantique',\n",
        "        'https://pt.wikipedia.org/wiki/Oliver_Typewriter_Company',\n",
        "        'https://pt.wikipedia.org/wiki/P%C3%A2nico_financeiro_de_1907',\n",
        "        'https://pt.wikipedia.org/wiki/Harvey_Milk',\n",
        "        'https://pt.wikipedia.org/wiki/Barack_Obama',\n",
        "        'https://pt.wikipedia.org/wiki/Ambientalismo',\n",
        "        'https://pt.wikipedia.org/wiki/Brian_Clough',\n",
        "        'https://pt.wikipedia.org/wiki/Ant%C3%A1rtida',\n",
        "        'https://pt.wikipedia.org/wiki/Fran%C3%A7a',\n",
        "        'https://pt.wikipedia.org/wiki/Ilhas_Malvinas',\n",
        "        'https://pt.wikipedia.org/wiki/Bilbau',\n",
        "        'https://pt.wikipedia.org/wiki/Londres',\n",
        "        'https://pt.wikipedia.org/wiki/Pedra_de_Roseta',\n",
        "        'https://pt.wikipedia.org/wiki/Edward_Smith',\n",
        "        'https://pt.wikipedia.org/wiki/Ulysses_(poema)',\n",
        "        'https://pt.wikipedia.org/wiki/L%C3%ADnguas_maias',\n",
        "        'https://pt.wikipedia.org/wiki/Hist%C3%B3ria_dos_conceitos',\n",
        "        'https://pt.wikipedia.org/wiki/Hamlet',\n",
        "        'https://pt.wikipedia.org/wiki/Vantagem_do_primeiro_movimento_no_xadrez',\n",
        "        'https://pt.wikipedia.org/wiki/Sistema_de_coordenadas_cartesiano'\n",
        "        'http://www.gutenberg.org/cache/epub/34387/pg34387.txt',                  #história de portugal\n",
        "        'http://www.gutenberg.org/cache/epub/55752/pg55752.txt',                  #Dom Casmurro\n",
        "        'http://www.gutenberg.org/cache/epub/54829/pg54829.txt',                  #Memórias póstumas Braz...\n",
        "        'http://www.gutenberg.org/cache/epub/3333/pg3333.txt',                    #Os Lusíadas\n",
        "        'http://www.gutenberg.org/cache/epub/31509/pg31509.txt',                  #obras completas camões\n",
        "        'http://www.gutenberg.org/cache/epub/55682/pg55682.txt',                  #quincas borba\n",
        "        'https://www.gutenberg.org/files/15047/15047-0.txt',                      #bases da ortografia portuguesa\n",
        "        'http://www.gutenberg.org/cache/epub/25641/pg25641.txt',                  #cartas de inglaterra\n",
        "        'https://www.gutenberg.org/ebooks/42942',                                 #primo basilio\n",
        "        'http://www.gutenberg.org/cache/epub/33056/pg33056.txt',                  #história sem data\n",
        "        'https://www.gutenberg.org/files/61653/61653-0.txt',                      #poesias completas machado\n",
        "        'https://epocanegocios.globo.com/Empresa/noticia/2020/10/stf-mantem-extradicao-de-ex-socio-da-telexfree-para-os-estados-unidos.html',\n",
        "        'https://g1.globo.com/rj/rio-de-janeiro/noticia/2020/10/20/witzel-usa-trechos-iguais-ao-da-wikipedia-em-defesa-entregue-a-tribunal-que-julga-impeachment.ghtml',\n",
        "        'https://g1.globo.com/economia/noticia/2020/10/20/acordo-preve-credito-de-us-1-bilhao-dos-eua-para-financiar-projetos-no-brasil-incluindo-5g.ghtml',\n",
        "        'https://globoesporte.globo.com/futebol/futebol-internacional/liga-dos-campeoes/jogo/20-10-2020/barcelona-ferencvarosi.ghtml',\n",
        "        'https://noticias.uol.com.br/politica/ultimas-noticias/2020/10/20/alexandre-de-moraes-e-novo-relator-do-inquerito-contra-bolsonaro-no-stf.htm',\n",
        "        'https://www.uol.com.br/esporte/futebol/ultimas-noticias/2020/10/20/diego-explica-golaco-contra-o-corinthians-e-diz-que-filipe-luis-e-nojento.htm',\n",
        "        'https://noticias.uol.com.br/politica/ultimas-noticias/2020/10/20/tse-determina-divulgacao-na-internet-de-extratos-bancarios-dos-partidos.htm',\n",
        "        'https://www.uol.com.br/splash/noticias/ooops/2020/10/20/globo-fecha-patrocinio-milionario-de-3-anos-para-novela-das-9.htm',\n",
        "        'https://www.uol.com.br/esporte/futebol/ultimas-noticias/2020/10/20/vice-do-atletico-mg-confirma-1-mes-de-atraso-salarial-mas-projeta-quitacao.htm',\n",
        "        'https://rollingstone.uol.com.br/noticia/por-que-winona-ryder-teve-medo-de-trabalhar-com-johnny-depp-em-edward-maos-de-tesoura/',\n",
        "        'https://noticias.uol.com.br/ultimas-noticias/efe/2020/10/20/reino-unido-registra-maior-numero-de-mortes-por-covid-19-desde-junho.htm',\n",
        "        'https://economia.uol.com.br/cotacoes/noticias/redacao/2020/10/20/fechamento-dolar-ibovespa-20-outubro.htm',\n",
        "        'https://globoesporte.globo.com/futebol/futebol-internacional/noticia/cristiano-ronaldo-depende-de-teste-nesta-quarta-para-ser-liberado-para-reencontro-com-messi-na-champions.ghtml',\n",
        "        'https://g1.globo.com/mundo/eleicoes-nos-eua/2020/noticia/2020/10/21/apos-manter-se-discreto-obama-estreara-em-campanha-de-biden-a-casa-branca.ghtml',\n",
        "        'https://epocanegocios.globo.com/Brasil/noticia/2020/10/coronavirus-na-contramao-do-mundo-brasil-segue-sem-restricoes-entrada-de-estrangeiros-por-aeroportos.html',\n",
        "        'https://www.techtudo.com.br/noticias/2020/10/iphone-12-vendeu-o-dobro-do-iphone-11-na-fase-de-encomenda-diz-analista.ghtml',\n",
        "        'https://www.uol.com.br/esporte/futebol/colunas/marcel-rizzo/2020/10/21/o-guru-de-miguel-ramirez-que-fez-o-tecnico-dizer-nao-ao-palmeiras-agora.htm',\n",
        "        'https://noticias.uol.com.br/cotidiano/ultimas-noticias/2020/10/20/promotor-suborno-desembargador-rio-de-janeiro.htm',\n",
        "        'https://www.uol.com.br/esporte/ultimas-noticias/2020/10/20/discordancia-de-caio-e-cleber-e-mais-uma-ao-vivo-da-dupla-relembre-outras.htm',\n",
        "        'https://www.uol.com.br/esporte/futebol/ultimas-noticias/2020/10/21/pai-de-pele-foi-a-aconselhado-a-encaminha-lo-a-concurso-no-banco-do-brasil.htm',\n",
        "        'https://noticias.uol.com.br/cotidiano/ultimas-noticias/2020/10/21/pf-faz-operacao-para-apurar-suspeita-de-fraudes-em-licitacoes-da-infraero.htm',\n",
        "        'https://noticias.uol.com.br/eleicoes/2020/10/20/motorista-de-primeira-dama-de-curitiba-morre-de-covid-19-casal-teve-doenca.htm',\n",
        "        'https://noticias.uol.com.br/politica/ultimas-noticias/2020/10/20/justica-decreta-indisponibilidade-dos-bens-de-eduardo-paes.htm',\n",
        "\n",
        "]\n",
        "\n",
        "text_col = 'bons_textos_div_wiki.txt'\n",
        "dic_name = 'pseudo_dic.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBOdoHWwRzPy"
      },
      "source": [
        "#scraper para inicializar o dicionário\n",
        "#searching tags\n",
        "#t_list = ['h1', 'h2', 'h3', 'p', 'a', 'ul', 'span', 'input']\n",
        "t_list = ['p', 'pre']\n",
        "\n",
        "with open(text_col, 'w', encoding='utf-8') as outfile:\n",
        "  for url in urls:\n",
        "    website = requests.get(url)\n",
        "    soup = BeautifulSoup(website.content, 'lxml')\n",
        "    tags = soup.find_all(t_list)\n",
        "    text = [' '.join(s.findAll(text=True)) for s in tags]\n",
        "    text_len = len(text)\n",
        "    for item in text:\n",
        "      print(item,file=outfile)\n",
        "\n",
        "with open(dic_name,'w') as outfile:\n",
        "  words = w_tokenize(text_col) + nltk.corpus.stopwords.words('portuguese') + ['autenticação', 'executa', 'fisicamente', 'ortogonalmente'] + 10*['Brasil', 'Brasília']\n",
        "  words = [re.sub(r'^(\\W+)?(\\w+)(\\W+)?',r'\\2',w) if re.match('^(\\W+)\\w+(\\W+)?',w) else w for w in words]\n",
        "  words = [w for w in words if len(w) > 2 or w in nltk.corpus.stopwords.words('portuguese')]\n",
        "  w_freq = fdist(words)\n",
        "  for w,f in w_freq.most_common():\n",
        "    print(w,file=outfile)\n",
        "\n",
        "dict_words = words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1yALPS5SPSz"
      },
      "source": [
        "!gzip -f pseudo_dic.txt\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMXi113V9dd"
      },
      "source": [
        "lm = LanguageModel('pseudo_dic.txt.gz')\n",
        "\n",
        "def filt_sentences(sentences,dictionary=dict_words):\n",
        "  filt_sent = []\n",
        "  for sentence in sentences:\n",
        "    d_words = [w for w in sentence.split(' ') if w.lower() in dictionary]\n",
        "    if len(d_words)/len(sentence.split(' ')) >= .5 and len(d_words) > 1:\n",
        "      filt_sent.append(sentence)\n",
        "  return filt_sent\n",
        "\n",
        "pp_tokens = []\n",
        "pt_tokens = []\n",
        "sentences = []\n",
        "raw_sentences = []\n",
        "for filename in f_pplumber:\n",
        "  pp_tokens.append((filename[:-4],w_tokenize(filename)))\n",
        "for filename in f_ptesseract:\n",
        "  pt_tokens.append((filename[:-4],w_tokenize(filename)))\n",
        "for t_name,tokens in pt_tokens:\n",
        "  raw_sents = build_sents(tokens)\n",
        "  aux_sent = filt_sentences(raw_sents)\n",
        "  raw_sentences.append((t_name, aux_sent))\n",
        "  n_tokens = []\n",
        "  for sentence in aux_sent:\n",
        "    for t in sentence.split(' '):\n",
        "      punctuation = None\n",
        "      if not (t[-1].isalpha() or t[-1].isdigit()):\n",
        "        punctuation = t[-1]\n",
        "      aux = lm.split(t)\n",
        "      for i,val in enumerate(aux):\n",
        "        if (i+1 == len(aux)) and punctuation:\n",
        "          n_tokens.append(val + punctuation)\n",
        "        else:\n",
        "          n_tokens.append(val)\n",
        "  sentences.append((t_name,build_sents(n_tokens)))\n",
        "#for name,tokens in pp_tokens, pt_tokens, sentences:\n",
        " # print(name); print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoF1i760bfd4"
      },
      "source": [
        "#words statistics\n",
        "from nltk import FreqDist as fdist\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import argsort, average\n",
        "from gensim.models import TfidfModel\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "#stop words\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "#this \"normalizes\" if-idf\n",
        "def init_ifidf_ref():\n",
        "  with open(text_col,'r') as infile:\n",
        "    text = None\n",
        "    for line in infile:\n",
        "      if not line == '\\n':\n",
        "        if text:\n",
        "          text += line\n",
        "        else:\n",
        "          text = line\n",
        "  return text.split()\n",
        "\n",
        "#defining lexical diversity function\n",
        "def lex_diversity(text):\n",
        "  return len(set(text))/len(text)\n",
        "\n",
        "#rate sentences\n",
        "def raw_imp_sent(text_sent, freqs):\n",
        "  imp_sents = defaultdict(int)\n",
        "  for sentence in text_sent:\n",
        "    sent_w = [re.sub(r'^(\\W+)?(\\w+)(\\W+)?',r'\\2',w) if re.match('^(\\W+)\\w+(\\W+)?',w) else w for w in sentence]\n",
        "    for w in sent_w:\n",
        "      if w in freqs:\n",
        "        imp_sents[sentence] += freqs[w]\n",
        "  return sorted(imp_sents.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#tf-idf   ajeitar para ele usar os textos\n",
        "def tfidf_imp_sent(text_sent):\n",
        "  processed_corpus = [[word for word in document.lower().split() if word not in stopwords] for document in text_sent]\n",
        "  dictionary = corpora.Dictionary(processed_corpus)\n",
        "  BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
        "  imp_sents = defaultdict(int)\n",
        "  for doc,sent in zip(tfidf[BoW_corpus],text_sent):\n",
        "    for id,freq in doc:\n",
        "      imp_sents[sent] += freq\n",
        "  return sorted(imp_sents.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#get file statistics\n",
        "def get_statistics(filename,w_tokens,s_tokens, raw_s_tokes=None):\n",
        "  ld = lex_diversity(w_tokens)\n",
        "  rel_words = [w.lower() for w in w_tokens if w.lower() not in stopwords]\n",
        "  rw_freqs = fdist(rel_words)\n",
        "  r_imp_sents = raw_imp_sent(s_tokens,rw_freqs)\n",
        "  tdidf_imp_sents = tfidf_imp_sent(s_tokens)\n",
        "  if raw_s_tokes:\n",
        "    raw_r_imp_sents = raw_imp_sent(raw_s_tokens, rw_freqs)\n",
        "    raw_tdidf_imp_sents = tfidf_imp_sent(raw_s_tokens)\n",
        "    return (filename,ld,rw_freqs,r_imp_sents,tdidf_imp_sents, raw_r_imp_sents, raw_tdidf_imp_sents)\n",
        "  return (filename,ld,rw_freqs,r_imp_sents,tdidf_imp_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWQ6ZbG_cJxg"
      },
      "source": [
        "results = []\n",
        "s_tokens = raw_s_tokens = None\n",
        "for filename, w_tokens in pp_tokens:\n",
        "  filename = filename[:-len('_pplumber')]     #criar uma variavel q guarda esse label\n",
        "  for name, sentence in sentences:\n",
        "    name = name[:-len('_ptesseract')]\n",
        "    if filename == name:\n",
        "      s_tokens = sentence\n",
        "      break\n",
        "  if not s_tokens:\n",
        "    print('An error occurred!!!!')\n",
        "    break\n",
        "  for name, sentence in raw_sentences:\n",
        "    name = name[:-len('_ptesseract')]\n",
        "    if filename == name:\n",
        "      raw_s_tokens = sentence\n",
        "      break\n",
        "  if not raw_s_tokens:\n",
        "    print('An error occurred!!!!')\n",
        "    break\n",
        "  results.append(get_statistics(filename,w_tokens,s_tokens,raw_s_tokens))\n",
        "\n",
        "t_most = 50\n",
        "t_sent = 5\n",
        "\n",
        "for filename,ld,rw_freqs,r_imp,tfidf_imp,raw_r_imp,raw_tfidf_imp in results:  \n",
        "  print('-'*100)\n",
        "  print('Estatísticas do arquivo \"{}\":'.format(filename))\n",
        "  print('-'*100)\n",
        "  print('Diversidade Lexical: {}'.format(ld))\n",
        "  print('As {} palavras relevantes mais comuns:\\n\\t{}'.format(t_most,rw_freqs.most_common(t_most)))\n",
        "  rw_freqs.plot(t_most,cumulative=True,title='{} palavras mais comuns'.format(t_most))\n",
        "  \n",
        "  print('As {} frases mais importantes (baseado na frequência -- tratado):'.format(t_sent))\n",
        "  for i,(s,val) in enumerate(r_imp[:t_sent]):\n",
        "    print('\\t(#{:03d}): \"{}\"'.format(i,s))\n",
        "  print('As {} frases mais importantes (baseado no tf-idf -- tratado):'.format(t_sent))\n",
        "  for i,s in enumerate(tfidf_imp[:t_sent]):\n",
        "    print('\\t(#{:03d}): \"{}\"'.format(i,s[0]))\n",
        "  print('As {} frases mais importantes (baseado na frequência -- não tratado):'.format(t_sent))\n",
        "  for i,(s,val) in enumerate(raw_r_imp[:t_sent]):\n",
        "    print('\\t(#{:03d}): \"{}\"'.format(i,s))\n",
        "  print('As {} frases mais importantes (baseado no tf-idf -- não tratado):'.format(t_sent))\n",
        "  for i,s in enumerate(raw_tfidf_imp[:t_sent]):\n",
        "    print('\\t(#{:03d}): \"{}\"'.format(i,s[0]))\n",
        "  print('-'*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}