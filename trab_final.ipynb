{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trab_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNkmhGcfMseUpEg8FWc52sE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2wKDI4xcRaL"
      },
      "source": [
        "!pip install numpy nltk matplotlib pdfplumber sklearn pdf2image pytesseract Image gensim\n",
        "!apt install tesseract-ocr tesseract-ocr-por poppler-utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYbptTrxjyCL"
      },
      "source": [
        "#incializando funções\n",
        "\n",
        "from pdf2image import convert_from_path as cPdf2img\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "from pytesseract import image_to_string as img2str\n",
        "import re\n",
        "\n",
        "#################################################\n",
        "page_id = '-'*100+'\\nPage {}\\n'+'-'*100+'\\n'\n",
        "\n",
        "#regex expressions\n",
        "regexes = [\n",
        "  [r'(\\n)([a-z]|-)', r'\\2'],\n",
        "  [r'(\\s+)(\\.|,|;)', r'\\2'],\n",
        "  [r'\\s+', r' ']\n",
        "]\n",
        "##################################################\n",
        "\n",
        "#usando pdfplumber\n",
        "def usePdfplumber(filename, verbose=True, n_pages=None):\n",
        "  if verbose:\n",
        "    print('Converting \"{}\" using pdfplumber'.format(filename))\n",
        "  fd = open(filename[:-4] + '_pplumber.txt', 'w')\n",
        "  with pdfplumber.open(filename) as pdf:\n",
        "    n_pages = len(pdf.pages)\n",
        "    for page,i in zip(pdf.pages,range(n_pages)):\n",
        "      if n_pages and i > n_pages:\n",
        "        break\n",
        "      print(\"Converting page #{}\".format(i+1))\n",
        "      text = page.extract_text()\n",
        "      for rPattern,subString in regexes:\n",
        "        text = re.sub(rPattern,subString,text)\n",
        "      fd.write(page_id.format(i+1))\n",
        "      fd.write(text + '\\n')\n",
        "  fd.close()\n",
        "\n",
        "#usando pytesseract\n",
        "def usePytesseract(filename, verbose=True, n_pages=None):\n",
        "  if verbose:\n",
        "    print('Converting \"{}\" using pytesseract'.format(filename))\n",
        "  fd = open(filename[:-4] + '_ptesseract.txt', 'w')\n",
        "  pages =cPdf2img(filename)\n",
        "  for i,page in enumerate(pages):\n",
        "    if n_pages and i > n_pages:\n",
        "      break\n",
        "    print(\"Converting page #{}\".format(i+1))\n",
        "    text = img2str(page, lang='por')\n",
        "    for rPattern,subString in regexes:\n",
        "        text = re.sub(rPattern,subString,text)\n",
        "    fd.write(page_id.format(i+1))\n",
        "    fd.write(text + '\\n')\n",
        "  fd.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSqNQnqrf0Xl"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbD4OMjplXHx"
      },
      "source": [
        "import os\n",
        "\n",
        "zipDir = '/gdrive/My Drive/Dataset/'\n",
        "zip_ref = zipfile.ZipFile(zipDir + 'Processos.zip', 'r')\n",
        "\n",
        "w_directory = 'processos'\n",
        "og_dir = os.getcwd()\n",
        "try:\n",
        "  os.mkdir(w_directory)\n",
        "except:\n",
        "  pass\n",
        "os.chdir(w_directory)\n",
        "\n",
        "zip_ref.extractall('.')\n",
        "zip_ref.close()\n",
        "\n",
        "os.chdir(og_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU36miibsXfP"
      },
      "source": [
        "proc_dir = og_dir + '/' + w_directory + '/Processos'\n",
        "print(proc_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfqhW-1jmng5"
      },
      "source": [
        "aux = [dirname for dirname in os.listdir(proc_dir) if os.path.isdir(os.path.join(proc_dir,dirname))]\n",
        "proc_dirs = [proc_dir + '/' + dirname for dirname in aux]\n",
        "del aux\n",
        "print(proc_dirs[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIk63WNXkE6G"
      },
      "source": [
        "for proc_dirname in proc_dirs[100:105]:\n",
        "  files = [proc_dirname + '/' + filename for filename in os.listdir(proc_dirname) if filename[-3:] == 'pdf']\n",
        "  for filename in files:\n",
        "    #usePdfplumber(filename, n_pages=2)\n",
        "    usePytesseract(filename, n_pages=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xBxzO27oANe"
      },
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/gdrive/My Drive/Aux/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6RF7XS8vbJr"
      },
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "procs = {}\n",
        "for dirname in proc_dirs[100:105]:\n",
        "  print('Listing txt files from \"{}\"'.format(dirname))\n",
        "  f_txt = [filename for filename in os.listdir(dirname) if filename[-4:] == \".txt\"]\n",
        "  print(f_txt)\n",
        "  procs[dirname] = f_txt\n",
        "\n",
        "for dirname in procs.keys():\n",
        "  aux_name = dirname.split('/')[-1]\n",
        "  try:\n",
        "    os.mkdir(base_dir + aux_name)\n",
        "  except:\n",
        "    pass\n",
        "  print('Copying files from {}...'.format(dirname))\n",
        "  for txt_file in procs[dirname]:\n",
        "    print('...copying {}...'.format(txt_file))\n",
        "    source = dirname + '/' + txt_file\n",
        "    dest = base_dir + aux_name + '/' + txt_file\n",
        "    copyfile(source,dest)\n",
        "  print('...done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbkBKSiunuD5"
      },
      "source": [
        "#tokenizers\n",
        "\n",
        "#word tokenization\n",
        "def w_tokenize(filename):\n",
        "  fd = open(filename,'r')\n",
        "  text = None\n",
        "  for line in fd:\n",
        "    if not re.search('-{2,}',line) and not re.match('Page\\ [0-9]+$',line):\n",
        "      if text:\n",
        "        text += line\n",
        "      else:\n",
        "        text = line\n",
        "  fd.close()\n",
        "  try:\n",
        "    return text.split()\n",
        "  except:\n",
        "    return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkzA8rgKqYia"
      },
      "source": [
        "aux_dirs = [os.path.join(base_dir,dirname) for dirname in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir,dirname))]\n",
        "\n",
        "classes = ['Despejo', \n",
        "'Embargos à Execução',\n",
        "'Execução de Título Extrajudicial',\n",
        "'Fato do Serviço',\n",
        "'Impugnação ao Cumprimento de Sentença',\n",
        "'Indenização e Obrigação de Fazer',\n",
        "'Interdito Proibitório',\n",
        "'Liquidação por Arbitramento',\n",
        "'Nulidade de Assembleia',\n",
        "'Obrigação de Fazer',\n",
        "'Plano de Saúde',\n",
        "'Poupança, Espurgos Inflacionários',\n",
        "'Prestação de Contas',\n",
        "'Prestação de Serviço',\n",
        "'Reintegração de Posse de Arrendamento Mercantil',\n",
        "'Responsabilidade do Fornecedor',\n",
        "'Revisão de Cláusula Contratual',\n",
        "'Suplementação Previdenciária',\n",
        "'Sustação de Protesto',\n",
        "'TOI',\n",
        "'Usucapião']\n",
        "\n",
        "busca_classes = list()\n",
        "for val in classes:\n",
        "  aux = val.split()\n",
        "  busca_classes += [word for word in aux if word not in ['de', 'à', 'ao', 'e', 'do']]\n",
        "\n",
        "files = {}\n",
        "for dirname in aux_dirs:\n",
        "  f_txt = [filename for filename in os.listdir(dirname) if filename[-4:] == \".txt\"]\n",
        "  words = list()\n",
        "  print('Palavras de {}'.format(dirname))\n",
        "  for filename in f_txt:\n",
        "    words += w_tokenize(dirname + '/' + filename)\n",
        "  all_caps = [word for word in words if re.match('[A-ZÂÃÍÓÕÚÇ]+',word)]\n",
        "  print(all_caps)\n",
        "  clss = [word for word in all_caps if word in busca_classes]\n",
        "  print(clss)\n",
        "  c_count = [0 for i in range(len(classes)+1)]\n",
        "  if len(clss):\n",
        "    for i, c_name in enumerate(classes):\n",
        "      for word in clss:\n",
        "        if word in c_name.split():\n",
        "          c_count[i] += 1\n",
        "    print(classes[c_count.index(max(c_count))])\n",
        "  else:\n",
        "    c_count[-1] = 1\n",
        "    print('Outros')\n",
        "  print(c_count)\n",
        "  print('-'*10)\n",
        "  text = ' '.join(words)\n",
        "  files[dirname] = (text,[1 if i == c_count.index(max(c_count)) else 0 for i in range(len(c_count))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCm0hilJJex6"
      },
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAuUPLVT9luK"
      },
      "source": [
        "import nltk\n",
        "#from gensim import corpora\n",
        "#from gensim.models import TfidfModel\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_size = 1000\n",
        "#stop words\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "aux = [[word.lower() for word in text if word.lower() not in stopwords] for keys,(text,_) in files.items()]\n",
        "processed_corpus = [' '.join(sentence) for sentence in aux]\n",
        "\n",
        "from collections import Counter\n",
        "def counter_word(text):\n",
        "  count = Counter()\n",
        "  for sentence in text:\n",
        "    for word in sentence.split():\n",
        "      count[word] += 1\n",
        "  return count\n",
        "\n",
        "counter = counter_word(processed_corpus)\n",
        "num_words = len(counter)\n",
        "#dictionary = corpora.Dictionary(processed_corpus)\n",
        "#BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
        "#tfidf_map = TfidfModel(BoW_corpus, smartirs='ntc')\n",
        "#X = []; y = []\n",
        "#for keys, (text,clss) in files.items():\n",
        " # processed_text = [word.lower() for word in text if word.lower() not in stopwords]\n",
        " # if len(processed_text) < max_size:\n",
        " #   processed_text += ['NaN' for i in range(max_size-len(processed_text))]\n",
        " # p_input = [dictionary.doc2bow([text]) for text in processed_text]\n",
        "#  for i,j in zip(p_input, tfidf_map[p_input[:max_size]]):\n",
        "#    print(i); print(j)\n",
        "#    input()\n",
        "#  X.append(tfidf_map[p_input[:max_size]])\n",
        "#  y.append(clss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6IquqNmvn6a"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(processed_corpus)\n",
        "\n",
        "X = [] \n",
        "y = []\n",
        "for keys, (text,clss) in files.items():\n",
        "  processed_text = [word.lower() for word in text.split() if word.lower() not in stopwords + ['%', '\"', \"!\", \"'\", '\"', '&', \"%,\"]]\n",
        "  processed_text = tokenizer.texts_to_sequences(' '.join(processed_text))\n",
        "  aux = []\n",
        "  for vals in processed_text:\n",
        "    aux += [i for i in vals]\n",
        "  processed_text = aux[:max_size]\n",
        "  if len(processed_text) < max_size:\n",
        "    processed_text += [0 for i in range(max_size - len(processed_text))]\n",
        "  X.append(processed_text)\n",
        "  y.append(clss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDoZ5LeJMJ5O"
      },
      "source": [
        "#rede bi-lstm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Embedding\n",
        "from tensorflow import convert_to_tensor\n",
        "from numpy import array\n",
        "\n",
        "n_classes = len(y[0])\n",
        "epochs = 10\n",
        "\n",
        "blstm_model = Sequential()\n",
        "blstm_model.add(Embedding(num_words,32, input_length=max_size))\n",
        "blstm_model.add(Bidirectional(LSTM(int(max_size/10))))                                    #20 memory units, (n_samp,1) means 10 time steps with a single feature\n",
        "blstm_model.add(Dense(n_classes,activation='softmax'))                                    #this TimeDistributed wrapper ensures that the lstm layer outputs a sequence of values (one per timestep) rather the a single value for the whole input sequence\n",
        "blstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "\n",
        "#train lstm\n",
        "X = array(X)\n",
        "y = array(y)\n",
        "blstm_model.fit(X,y, epochs=epochs, batch_size=10, verbose=True, validation_split=.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1VGD0eyFA93"
      },
      "source": [
        "procs = {}\n",
        "for dirname in proc_dirs[100:105]:\n",
        "  print('Listing txt files from \"{}\"'.format(dirname))\n",
        "  f_txt = [filename for filename in os.listdir(dirname) if filename[-4:] == \".txt\"]\n",
        "  print(f_txt)\n",
        "  procs[dirname] = f_txt\n",
        "  for filename in f_txt:\n",
        "    words += w_tokenize(dirname + '/' + filename)\n",
        "  words = ' '.join(words)\n",
        "  processed_text = [word.lower() for word in text.split() if word.lower() not in stopwords + ['%', '\"', \"!\", \"'\", '\"', '&', \"%,\"]]\n",
        "  processed_text = tokenizer.texts_to_sequences(' '.join(processed_text))\n",
        "  aux = []\n",
        "  for vals in processed_text:\n",
        "    aux += [i for i in vals]\n",
        "  processed_text = aux[:max_size]\n",
        "  if len(processed_text) < max_size:\n",
        "    processed_text += [0 for i in range(max_size - len(processed_text))]\n",
        "  print(classes[bilstm_model(processed_text,train=False)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}